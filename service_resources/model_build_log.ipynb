{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install autogluon.tabular[all]==0.8.2\n"
      ],
      "metadata": {
        "id": "1GRS_nKG6JQg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4084ba6b-16ce-49bc-bc2c-c415711f9b0f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autogluon.tabular[all]==0.8.2\n",
            "  Downloading autogluon.tabular-0.8.2-py3-none-any.whl (285 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.7/285.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<1.27,>=1.21 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]==0.8.2) (1.23.5)\n",
            "Requirement already satisfied: scipy<1.12,>=1.5.4 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]==0.8.2) (1.11.3)\n",
            "Requirement already satisfied: pandas<1.6,>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]==0.8.2) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn<1.3,>=1.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]==0.8.2) (1.2.2)\n",
            "Requirement already satisfied: networkx<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]==0.8.2) (3.1)\n",
            "Collecting autogluon.core==0.8.2 (from autogluon.tabular[all]==0.8.2)\n",
            "  Downloading autogluon.core-0.8.2-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.0/224.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting autogluon.features==0.8.2 (from autogluon.tabular[all]==0.8.2)\n",
            "  Downloading autogluon.features-0.8.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lightgbm<3.4,>=3.3 (from autogluon.tabular[all]==0.8.2)\n",
            "  Downloading lightgbm-3.3.5-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xgboost<1.8,>=1.6 (from autogluon.tabular[all]==0.8.2)\n",
            "  Downloading xgboost-1.7.6-py3-none-manylinux2014_x86_64.whl (200.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.3/200.3 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch<1.14,>=1.9 (from autogluon.tabular[all]==0.8.2)\n",
            "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fastai<2.8,>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]==0.8.2) (2.7.12)\n",
            "Collecting catboost<1.3,>=1.1 (from autogluon.tabular[all]==0.8.2)\n",
            "  Downloading catboost-1.2.2-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<5,>=4.38 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (4.66.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (2.31.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (3.7.1)\n",
            "Collecting boto3<2,>=1.10 (from autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2)\n",
            "  Downloading boto3-1.28.63-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting autogluon.common==0.8.2 (from autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2)\n",
            "  Downloading autogluon.common-0.8.2-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: hyperopt<0.2.8,>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (0.2.7)\n",
            "Collecting ray[default]<2.4,>=2.3 (from autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2)\n",
            "  Downloading ray-2.3.1-cp310-cp310-manylinux2014_x86_64.whl (58.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2.0,>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (1.10.13)\n",
            "Collecting grpcio<=1.50.0,>=1.42.0 (from autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2)\n",
            "  Downloading grpcio-1.50.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m109.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil<6,>=5.7.3 in /usr/local/lib/python3.10/dist-packages (from autogluon.common==0.8.2->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (5.9.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from autogluon.common==0.8.2->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (67.7.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]==0.8.2) (0.20.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]==0.8.2) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]==0.8.2) (1.16.0)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2) (23.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2) (23.2)\n",
            "Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2) (0.0.7)\n",
            "Requirement already satisfied: fastcore<1.6,>=1.5.29 in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2) (1.5.29)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2) (0.15.2+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2) (6.0.1)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2) (1.0.3)\n",
            "Requirement already satisfied: pillow>6.0.0 in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2) (9.4.0)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2) (3.6.1)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from lightgbm<3.4,>=3.3->autogluon.tabular[all]==0.8.2) (0.41.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<1.6,>=1.4.1->autogluon.tabular[all]==0.8.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<1.6,>=1.4.1->autogluon.tabular[all]==0.8.2) (2023.3.post1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.3,>=1.0->autogluon.tabular[all]==0.8.2) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.3,>=1.0->autogluon.tabular[all]==0.8.2) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<1.14,>=1.9->autogluon.tabular[all]==0.8.2) (4.5.0)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch<1.14,>=1.9->autogluon.tabular[all]==0.8.2)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch<1.14,>=1.9->autogluon.tabular[all]==0.8.2)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch<1.14,>=1.9->autogluon.tabular[all]==0.8.2)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch<1.14,>=1.9->autogluon.tabular[all]==0.8.2)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.32.0,>=1.31.63 (from boto3<2,>=1.10->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2)\n",
            "  Downloading botocore-1.31.63-py3-none-any.whl (11.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m106.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3<2,>=1.10->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.8.0,>=0.7.0 (from boto3<2,>=1.10->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2)\n",
            "  Downloading s3transfer-0.7.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (0.18.3)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (2.2.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (0.10.9.7)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (23.1.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (3.12.4)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (4.19.1)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (1.0.7)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (3.20.3)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (1.4.0)\n",
            "Collecting virtualenv>=20.0.24 (from ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2)\n",
            "  Downloading virtualenv-20.24.5-py3-none-any.whl (3.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp>=3.7 in /usr/local/lib/python3.10/dist-packages (from ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (3.8.6)\n",
            "Collecting aiohttp-cors (from ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2)\n",
            "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
            "Collecting colorful (from ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2)\n",
            "  Downloading colorful-0.5.5-py2.py3-none-any.whl (201 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting py-spy>=0.2.0 (from ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2)\n",
            "  Downloading py_spy-0.3.14-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gpustat>=1.0.0 (from ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2)\n",
            "  Downloading gpustat-1.1.1.tar.gz (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.1/98.1 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting opencensus (from ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2)\n",
            "  Downloading opencensus-0.11.3-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (0.17.1)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.10/dist-packages (from ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (6.4.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (0.9.0)\n",
            "Collecting tensorboardX>=1.9 (from ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2) (0.10.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2) (3.1.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2) (3.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (2023.7.22)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision>=0.8.2 (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2)\n",
            "  Downloading torchvision-0.16.0-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m110.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m118.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading torchvision-0.15.1-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading torchvision-0.14.1-cp310-cp310-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (3.1.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost<1.3,>=1.1->autogluon.tabular[all]==0.8.2) (8.2.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (1.9.2)\n",
            "Collecting nvidia-ml-py>=11.450.129 (from gpustat>=1.0.0->ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2)\n",
            "  Downloading nvidia_ml_py-12.535.108-py3-none-any.whl (36 kB)\n",
            "Collecting blessed>=1.17.1 (from gpustat>=1.0.0->ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2)\n",
            "  Downloading blessed-1.20.0-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2) (0.1.3)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv>=20.0.24->ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2)\n",
            "  Downloading distlib-0.3.7-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs<4,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.0.24->ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (3.11.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2) (2.1.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (0.10.4)\n",
            "Collecting opencensus-context>=0.1.3 (from opencensus->ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2)\n",
            "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opencensus->ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (2.11.1)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (0.2.8)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (2.17.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]<2.4,>=2.3->autogluon.core==0.8.2->autogluon.tabular[all]==0.8.2) (0.5.0)\n",
            "Building wheels for collected packages: gpustat\n",
            "  Building wheel for gpustat (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpustat: filename=gpustat-1.1.1-py3-none-any.whl size=26534 sha256=5262d9d438f892c9541e5b2833c9bd4ec6a290fbf202188ef078b4e836e6cea7\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/d7/80/a71ba3540900e1f276bcae685efd8e590c810d2108b95f1e47\n",
            "Successfully built gpustat\n",
            "Installing collected packages: py-spy, opencensus-context, nvidia-ml-py, distlib, colorful, virtualenv, tensorboardX, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, jmespath, grpcio, blessed, xgboost, nvidia-cudnn-cu11, gpustat, botocore, torch, s3transfer, lightgbm, catboost, aiohttp-cors, torchvision, ray, opencensus, boto3, autogluon.common, autogluon.features, autogluon.core, autogluon.tabular\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.59.0\n",
            "    Uninstalling grpcio-1.59.0:\n",
            "      Successfully uninstalled grpcio-1.59.0\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 2.0.0\n",
            "    Uninstalling xgboost-2.0.0:\n",
            "      Successfully uninstalled xgboost-2.0.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1+cu118\n",
            "    Uninstalling torch-2.0.1+cu118:\n",
            "      Successfully uninstalled torch-2.0.1+cu118\n",
            "  Attempting uninstall: lightgbm\n",
            "    Found existing installation: lightgbm 4.0.0\n",
            "    Uninstalling lightgbm-4.0.0:\n",
            "      Successfully uninstalled lightgbm-4.0.0\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.15.2+cu118\n",
            "    Uninstalling torchvision-0.15.2+cu118:\n",
            "      Successfully uninstalled torchvision-0.15.2+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiohttp-cors-0.7.0 autogluon.common-0.8.2 autogluon.core-0.8.2 autogluon.features-0.8.2 autogluon.tabular-0.8.2 blessed-1.20.0 boto3-1.28.63 botocore-1.31.63 catboost-1.2.2 colorful-0.5.5 distlib-0.3.7 gpustat-1.1.1 grpcio-1.50.0 jmespath-1.0.1 lightgbm-3.3.5 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-ml-py-12.535.108 opencensus-0.11.3 opencensus-context-0.1.3 py-spy-0.3.14 ray-2.3.1 s3transfer-0.7.0 tensorboardX-2.6.2.2 torch-1.13.1 torchvision-0.14.1 virtualenv-20.24.5 xgboost-1.7.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tApLOUtVKaQ",
        "outputId": "b50d6f26-f2ad-4940-db84-bb03c3bb8876"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "1T6YTpAM9dwl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6c5PQb4vzNUF"
      },
      "outputs": [],
      "source": [
        "\n",
        "from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "from autogluon.common.utils.utils import setup_outputdir\n",
        "from autogluon.core.utils.loaders import load_pkl\n",
        "from autogluon.core.utils.savers import save_pkl\n",
        "import os.path\n",
        "\n",
        "\"\"\"\n",
        "@author: Lyle\n",
        "\"\"\"\n",
        "\n",
        "class MultilabelPredictor:\n",
        "    \"\"\" Tabular Predictor for predicting multiple columns in table.\n",
        "        Creates multiple TabularPredictor objects which you can also use individually.\n",
        "        You can access the TabularPredictor for a particular label via: `multilabel_predictor.get_predictor(label_i)`\n",
        "\n",
        "        Parameters ---------- labels : List[str] The ith element of this list is the column (i.e. `label`) predicted\n",
        "        by the ith TabularPredictor stored in this object. path : str, default = None Path to directory where models\n",
        "        and intermediate outputs should be saved. If unspecified, a time-stamped folder called \"AutogluonModels/ag-[\n",
        "        TIMESTAMP]\" will be created in the working directory to store all models. Note: To call `fit()` twice and\n",
        "        save all results of each fit, you must specify different `path` locations or don't specify `path` at all.\n",
        "        Otherwise files from first `fit()` will be overwritten by second `fit()`. Caution: when predicting many\n",
        "        labels, this directory may grow large as it needs to store many TabularPredictors. problem_types : List[str],\n",
        "        default = None The ith element is the `problem_type` for the ith TabularPredictor stored in this object.\n",
        "        eval_metrics : List[str], default = None The ith element is the `eval_metric` for the ith TabularPredictor\n",
        "        stored in this object. consider_labels_correlation : bool, default = True Whether the predictions of multiple\n",
        "        labels should account for label correlations or predict each label independently of the others. If True,\n",
        "        the ordering of `labels` may affect resulting accuracy as each label is predicted conditional on the previous\n",
        "        labels appearing earlier in this list (i.e. in an auto-regressive fashion). Set to False if during inference\n",
        "        you may want to individually use just the ith TabularPredictor without predicting all the other labels.\n",
        "        kwargs : Arguments passed into the initialization of each TabularPredictor.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    multi_predictor_file = 'multilabel_predictor.pkl'\n",
        "\n",
        "    def __init__(self, labels, path=None, problem_types=None, eval_metrics=None, consider_labels_correlation=True,\n",
        "                 **kwargs):\n",
        "        self.model_root = None\n",
        "        if len(labels) < 2:\n",
        "            raise ValueError(\n",
        "                \"MultilabelPredictor is only intended for predicting MULTIPLE labels (columns), use TabularPredictor \"\n",
        "                \"for predicting one label (column).\")\n",
        "        if (problem_types is not None) and (len(problem_types) != len(labels)):\n",
        "            raise ValueError(\"If provided, `problem_types` must have same length as `labels`\")\n",
        "        if (eval_metrics is not None) and (len(eval_metrics) != len(labels)):\n",
        "            raise ValueError(\"If provided, `eval_metrics` must have same length as `labels`\")\n",
        "        self.path = setup_outputdir(path, warn_if_exist=False)\n",
        "        self.labels = labels\n",
        "        self.consider_labels_correlation = consider_labels_correlation\n",
        "        self.predictors = {}  # key = label, value = TabularPredictor or str path to the TabularPredictor for this label\n",
        "        if eval_metrics is None:\n",
        "            self.eval_metrics = {}\n",
        "        else:\n",
        "            self.eval_metrics = {labels[i]: eval_metrics[i] for i in range(len(labels))}\n",
        "        problem_type = None\n",
        "        eval_metric = None\n",
        "        for i in range(len(labels)):\n",
        "            label = labels[i]\n",
        "            path_i = self.path + \"Predictor_\" + label\n",
        "            if problem_types is not None:\n",
        "                problem_type = problem_types[i]\n",
        "            if eval_metrics is not None:\n",
        "                eval_metric = eval_metrics[i]\n",
        "            self.predictors[label] = TabularPredictor(label=label, problem_type=problem_type, eval_metric=eval_metric,\n",
        "                                                      path=path_i, **kwargs)\n",
        "\n",
        "    def fit(self, train_data, tuning_data=None, **kwargs):\n",
        "        \"\"\" Fits a separate TabularPredictor to predict each of the labels.\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            train_data, tuning_data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n",
        "                See documentation for `TabularPredictor.fit()`.\n",
        "            kwargs :\n",
        "                Arguments passed into the `fit()` call for each TabularPredictor.\n",
        "        \"\"\"\n",
        "        if isinstance(train_data, str):\n",
        "            train_data = TabularDataset(train_data)\n",
        "        if tuning_data is not None and isinstance(tuning_data, str):\n",
        "            tuning_data = TabularDataset(tuning_data)\n",
        "        train_data_og = train_data.copy()\n",
        "        if tuning_data is not None:\n",
        "            tuning_data_og = tuning_data.copy()\n",
        "        else:\n",
        "            tuning_data_og = None\n",
        "        save_metrics = len(self.eval_metrics) == 0\n",
        "        for i in range(len(self.labels)):\n",
        "            label = self.labels[i]\n",
        "            predictor = self.get_predictor(label)\n",
        "            if not self.consider_labels_correlation:\n",
        "                labels_to_drop = [l for l in self.labels if l != label]\n",
        "            else:\n",
        "                labels_to_drop = [self.labels[j] for j in range(i + 1, len(self.labels))]\n",
        "            train_data = train_data_og.drop(labels_to_drop, axis=1)\n",
        "            if tuning_data is not None:\n",
        "                tuning_data = tuning_data_og.drop(labels_to_drop, axis=1)\n",
        "            print(f\"Fitting TabularPredictor for label: {label} ...\")\n",
        "            predictor.fit(train_data=train_data, tuning_data=tuning_data, **kwargs)\n",
        "            self.predictors[label] = predictor.path\n",
        "            if save_metrics:\n",
        "                self.eval_metrics[label] = predictor.eval_metric\n",
        "        self.save()\n",
        "\n",
        "    def predict(self, data, **kwargs):\n",
        "        \"\"\" Returns DataFrame with label columns containing predictions for each label.\n",
        "\n",
        "            Parameters ---------- data : str or autogluon.tabular.TabularDataset or pd.DataFrame Data to make\n",
        "            predictions for. If label columns are present in this data, they will be ignored. See documentation for\n",
        "            `TabularPredictor.predict()`. kwargs : Arguments passed into the predict() call for each TabularPredictor.\n",
        "        \"\"\"\n",
        "        return self._predict(data, as_proba=False, **kwargs)\n",
        "\n",
        "    def predict_proba(self, data, **kwargs):\n",
        "        \"\"\" Returns dict where each key is a label and the corresponding value is the `predict_proba()` output for just that label.\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n",
        "                Data to make predictions for. See documentation for `TabularPredictor.predict()` and `TabularPredictor.predict_proba()`.\n",
        "            kwargs :\n",
        "                Arguments passed into the `predict_proba()` call for each TabularPredictor (also passed into a `predict()` call).\n",
        "        \"\"\"\n",
        "        return self._predict(data, as_proba=True, **kwargs)\n",
        "\n",
        "    def evaluate(self, data, **kwargs):\n",
        "        \"\"\" Returns dict where each key is a label and the corresponding value is the `evaluate()` output for just that label.\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n",
        "                Data to evalate predictions of all labels for, must contain all labels as columns. See documentation for `TabularPredictor.evaluate()`.\n",
        "            kwargs :\n",
        "                Arguments passed into the `evaluate()` call for each TabularPredictor (also passed into the `predict()` call).\n",
        "        \"\"\"\n",
        "        data = self._get_data(data)\n",
        "        eval_dict = {}\n",
        "        for label in self.labels:\n",
        "            print(f\"Evaluating TabularPredictor for label: {label} ...\")\n",
        "            predictor = self.get_predictor(label)\n",
        "            eval_dict[label] = predictor.evaluate(data, **kwargs)\n",
        "            if self.consider_labels_correlation:\n",
        "                data[label] = predictor.predict(data, **kwargs)\n",
        "        return eval_dict\n",
        "\n",
        "    def save(self):\n",
        "        \"\"\" Save MultilabelPredictor to disk. \"\"\"\n",
        "        for label in self.labels:\n",
        "            if not isinstance(self.predictors[label], str):\n",
        "                self.predictors[label] = self.predictors[label].path\n",
        "        save_pkl.save(path=self.path + self.multi_predictor_file, object=self)\n",
        "        print(f\"MultilabelPredictor saved to disk. Load with: MultilabelPredictor.load('{self.path}')\")\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, path):\n",
        "        \"\"\" Load MultilabelPredictor from disk `path` previously specified when creating this MultilabelPredictor. \"\"\"\n",
        "        predictor_instance = load_pkl.load(path=os.path.join(path, cls.multi_predictor_file))\n",
        "        predictor_instance.model_root = path\n",
        "        return predictor_instance\n",
        "\n",
        "    def get_predictor(self, label):\n",
        "        \"\"\" Returns TabularPredictor which is used to predict this label. \"\"\"\n",
        "        predictor = self.predictors[label]\n",
        "        if isinstance(predictor, str):\n",
        "            path_elements = predictor.split(\"/\")\n",
        "            path_relative_to_root = path_elements[-2] + \"/\" + path_elements[-1]\n",
        "            return TabularPredictor.load(path=os.path.join(self.model_root, path_relative_to_root))\n",
        "        return predictor\n",
        "\n",
        "    def _get_data(self, data):\n",
        "        if isinstance(data, str):\n",
        "            return TabularDataset(data)\n",
        "        return data.copy()\n",
        "\n",
        "    def _predict(self, data, as_proba=False, **kwargs):\n",
        "        data = self._get_data(data)\n",
        "        if as_proba:\n",
        "            predproba_dict = {}\n",
        "        for label in self.labels:\n",
        "            #             print(f\"Predicting with TabularPredictor for label: {label} ...\")\n",
        "            predictor = self.get_predictor(label)\n",
        "            if as_proba:\n",
        "                predproba_dict[label] = predictor.predict_proba(data, as_multiclass=True, **kwargs)\n",
        "            data[label] = predictor.predict(data, **kwargs)\n",
        "        if not as_proba:\n",
        "            return data[self.labels]\n",
        "        else:\n",
        "            return predproba_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sun Jul 10 12:05:43 2022\n",
        "\n",
        "@author: Lyle\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "ALL_STRUCTURAL_DATASET = \"/content/drive/MyDrive/all_structural_data_aug.csv\"\n",
        "\n",
        "\n",
        "def one_hot_encode_material(data):\n",
        "    data = data.copy()\n",
        "    # One-hot encode the materials\n",
        "    data.loc[:, \"Material\"] = pd.Categorical(data[\"Material\"], categories=[\"Steel\", \"Aluminum\", \"Titanium\"])\n",
        "    mats_oh = pd.get_dummies(data[\"Material\"], prefix=\"Material=\", prefix_sep=\"\")\n",
        "    data.drop([\"Material\"], axis=1, inplace=True)\n",
        "    data = pd.concat([mats_oh, data], axis=1)\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_augmented_framed_dataset():\n",
        "    reg_data = pd.read_csv(ALL_STRUCTURAL_DATASET, index_col=0)\n",
        "\n",
        "    x = reg_data.iloc[:, :-11]\n",
        "\n",
        "    x = one_hot_encode_material(x)\n",
        "\n",
        "    x, x_scaler = scale(x)\n",
        "    y = reg_data.iloc[:, -11:-1]\n",
        "\n",
        "    for col in ['Sim 1 Safety Factor', 'Sim 3 Safety Factor']:\n",
        "        y[col] = 1 / y[col]\n",
        "        y.rename(columns={col: col + \" (Inverted)\"}, inplace=True)\n",
        "    # THIS MODEL HAS BEEN TRAINED ON SIGNED DISPLACEMENT VALUES INSTEAD OF MAGNITUDES\n",
        "    # for col in ['Sim 1 Dropout X Disp.', 'Sim 1 Dropout Y Disp.', 'Sim 1 Bottom Bracket X Disp.',\n",
        "    #             'Sim 1 Bottom Bracket Y Disp.', 'Sim 2 Bottom Bracket Z Disp.', 'Sim 3 Bottom Bracket Y Disp.',\n",
        "    #             'Sim 3 Bottom Bracket X Rot.', 'Model Mass']:\n",
        "    #     y[col] = [np.abs(val) for val in y[col].values]\n",
        "    #     y.rename(columns={col: col + \" Magnitude\"}, inplace=True)\n",
        "    y, y_scaler = scale(y)\n",
        "\n",
        "    return x, y, x_scaler, y_scaler\n",
        "\n",
        "\n",
        "def scale(v):\n",
        "    v_scaler = StandardScaler()\n",
        "    v_scaler.fit(v)\n",
        "    v_scaled_values = v_scaler.transform(v)\n",
        "    new_v = pd.DataFrame(v_scaled_values, columns=v.columns, index=v.index)\n",
        "    return new_v, v_scaler"
      ],
      "metadata": {
        "id": "ClnOq0dr8mVk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_scaled, y_scaled, x_scaler, y_scaler = load_augmented_framed_dataset()\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y_scaled, random_state=2023)\n"
      ],
      "metadata": {
        "id": "G6QftljF9LHm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c439da2f-eea9-4f0a-afb4-c0af76d7a53c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-9467006776f7>:18: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  data.loc[:, \"Material\"] = pd.Categorical(data[\"Material\"], categories=[\"Steel\", \"Aluminum\", \"Titanium\"])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x_test), len(y_test))\n",
        "len(x_train), len(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "725XVLhbfqKu",
        "outputId": "96a388e2-776a-46d2-b095-648e46774ca3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3713 3713\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11138, 11138)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_training_set = pd.concat([x_train, y_train], axis=1)\n",
        "len(full_training_set)"
      ],
      "metadata": {
        "id": "xo-Tltexhttk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ec2d474-827e-4c49-e4e9-6b51b60aaa62"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11138"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_predictor = MultilabelPredictor(labels=y_scaled.columns)\n",
        "my_predictor.fit(\n",
        "    train_data=full_training_set, presets='optimize_for_deployment', num_gpus=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5-vSuEU9RtX",
        "outputId": "a6807294-66e7-4ed7-c0d7-ed496c34a175"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Presets specified: ['optimize_for_deployment']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Dropout X Disp./\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Wed Aug 30 11:19:59 UTC 2023\n",
            "Disk Space Avail:   48.58 GB / 83.96 GB (57.9%)\n",
            "Train Data Rows:    11138\n",
            "Train Data Columns: 39\n",
            "Label Column: Sim 1 Dropout X Disp.\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (12.31357753874557, -5.036857510156917, -0.00664, 0.99526)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    11857.5 MB\n",
            "\tTrain Data (Original)  Memory Usage: 3.48 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 5 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 39 | ['Material=Steel', 'Material=Aluminum', 'Material=Titanium', 'SSB_Include', 'CSB_Include', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 34 | ['CS Length', 'BB Drop', 'Stack', 'SS E', 'ST Angle', ...]\n",
            "\t\t('int', ['bool']) :  5 | ['Material=Steel', 'Material=Aluminum', 'Material=Titanium', 'SSB_Include', 'CSB_Include']\n",
            "\t0.1s = Fit runtime\n",
            "\t39 features in original data used to generate 39 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 3.09 MB (0.0% of available memory)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting TabularPredictor for label: Sim 1 Dropout X Disp. ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Data preprocessing and feature engineering runtime = 0.18s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 10024, Val Rows: 1114\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ...\n",
            "\t-0.7754\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.03s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ...\n",
            "\t-0.7507\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ...\n",
            "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t-0.3461\t = Validation score   (-root_mean_squared_error)\n",
            "\t5.81s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: LightGBM ...\n",
            "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t-0.3191\t = Validation score   (-root_mean_squared_error)\n",
            "\t2.29s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ...\n",
            "\t-0.3412\t = Validation score   (-root_mean_squared_error)\n",
            "\t74.41s\t = Training   runtime\n",
            "\t0.26s\t = Validation runtime\n",
            "Fitting model: CatBoost ...\n",
            "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
            "\t-0.3212\t = Validation score   (-root_mean_squared_error)\n",
            "\t7.63s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE ...\n",
            "\t-0.4121\t = Validation score   (-root_mean_squared_error)\n",
            "\t16.23s\t = Training   runtime\n",
            "\t0.15s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ...\n",
            "\t-0.3711\t = Validation score   (-root_mean_squared_error)\n",
            "\t17.64s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: XGBoost ...\n",
            "\t-0.3342\t = Validation score   (-root_mean_squared_error)\n",
            "\t2.21s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ...\n",
            "\t-0.346\t = Validation score   (-root_mean_squared_error)\n",
            "\t32.67s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ...\n",
            "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t-0.3199\t = Validation score   (-root_mean_squared_error)\n",
            "\t9.76s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-0.3089\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.53s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 171.86s ... Best model: \"WeightedEnsemble_L2\"\n",
            "Deleting model KNeighborsUnif. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Dropout X Disp./models/KNeighborsUnif/ will be removed.\n",
            "Deleting model KNeighborsDist. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Dropout X Disp./models/KNeighborsDist/ will be removed.\n",
            "Deleting model LightGBMXT. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Dropout X Disp./models/LightGBMXT/ will be removed.\n",
            "Deleting model RandomForestMSE. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Dropout X Disp./models/RandomForestMSE/ will be removed.\n",
            "Deleting model ExtraTreesMSE. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Dropout X Disp./models/ExtraTreesMSE/ will be removed.\n",
            "Deleting model XGBoost. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Dropout X Disp./models/XGBoost/ will be removed.\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Dropout X Disp./\")\n",
            "Presets specified: ['optimize_for_deployment']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Dropout Y Disp./\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Wed Aug 30 11:19:59 UTC 2023\n",
            "Disk Space Avail:   48.57 GB / 83.96 GB (57.9%)\n",
            "Train Data Rows:    11138\n",
            "Train Data Columns: 40\n",
            "Label Column: Sim 1 Dropout Y Disp.\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (4.873152389135353, -14.704521850848263, 0.00634, 0.96242)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    9693.38 MB\n",
            "\tTrain Data (Original)  Memory Usage: 3.56 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 5 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting TabularPredictor for label: Sim 1 Dropout Y Disp. ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 40 | ['Material=Steel', 'Material=Aluminum', 'Material=Titanium', 'SSB_Include', 'CSB_Include', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 35 | ['CS Length', 'BB Drop', 'Stack', 'SS E', 'ST Angle', ...]\n",
            "\t\t('int', ['bool']) :  5 | ['Material=Steel', 'Material=Aluminum', 'Material=Titanium', 'SSB_Include', 'CSB_Include']\n",
            "\t0.2s = Fit runtime\n",
            "\t40 features in original data used to generate 40 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 3.17 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.2s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 10024, Val Rows: 1114\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ...\n",
            "\t-0.533\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.03s\t = Training   runtime\n",
            "\t0.09s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ...\n",
            "\t-0.5214\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.03s\t = Training   runtime\n",
            "\t0.25s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ...\n",
            "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_set's rmse: 0.410868\n",
            "[2000]\tvalid_set's rmse: 0.407763\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-0.4077\t = Validation score   (-root_mean_squared_error)\n",
            "\t10.99s\t = Training   runtime\n",
            "\t0.24s\t = Validation runtime\n",
            "Fitting model: LightGBM ...\n",
            "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_set's rmse: 0.323699\n",
            "[2000]\tvalid_set's rmse: 0.312753\n",
            "[3000]\tvalid_set's rmse: 0.31383\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-0.3125\t = Validation score   (-root_mean_squared_error)\n",
            "\t18.54s\t = Training   runtime\n",
            "\t0.5s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ...\n",
            "\t-0.3175\t = Validation score   (-root_mean_squared_error)\n",
            "\t77.57s\t = Training   runtime\n",
            "\t0.26s\t = Validation runtime\n",
            "Fitting model: CatBoost ...\n",
            "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
            "\t-0.3315\t = Validation score   (-root_mean_squared_error)\n",
            "\t11.52s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE ...\n",
            "\t-0.3468\t = Validation score   (-root_mean_squared_error)\n",
            "\t16.85s\t = Training   runtime\n",
            "\t0.15s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ...\n",
            "No improvement since epoch 9: early stopping\n",
            "\t-0.2661\t = Validation score   (-root_mean_squared_error)\n",
            "\t11.84s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: XGBoost ...\n",
            "\t-0.338\t = Validation score   (-root_mean_squared_error)\n",
            "\t1.98s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ...\n",
            "\t-0.3661\t = Validation score   (-root_mean_squared_error)\n",
            "\t13.19s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ...\n",
            "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t-0.3523\t = Validation score   (-root_mean_squared_error)\n",
            "\t16.79s\t = Training   runtime\n",
            "\t0.22s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-0.2535\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.41s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 184.24s ... Best model: \"WeightedEnsemble_L2\"\n",
            "Deleting model KNeighborsUnif. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Dropout Y Disp./models/KNeighborsUnif/ will be removed.\n",
            "Deleting model KNeighborsDist. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Dropout Y Disp./models/KNeighborsDist/ will be removed.\n",
            "Deleting model LightGBMXT. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Dropout Y Disp./models/LightGBMXT/ will be removed.\n",
            "Deleting model RandomForestMSE. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Dropout Y Disp./models/RandomForestMSE/ will be removed.\n",
            "Deleting model CatBoost. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Dropout Y Disp./models/CatBoost/ will be removed.\n",
            "Deleting model ExtraTreesMSE. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Dropout Y Disp./models/ExtraTreesMSE/ will be removed.\n",
            "Deleting model XGBoost. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Dropout Y Disp./models/XGBoost/ will be removed.\n",
            "Deleting model NeuralNetTorch. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Dropout Y Disp./models/NeuralNetTorch/ will be removed.\n",
            "Deleting model LightGBMLarge. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Dropout Y Disp./models/LightGBMLarge/ will be removed.\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Dropout Y Disp./\")\n",
            "Presets specified: ['optimize_for_deployment']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Bottom Bracket X Disp./\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Wed Aug 30 11:19:59 UTC 2023\n",
            "Disk Space Avail:   48.57 GB / 83.96 GB (57.8%)\n",
            "Train Data Rows:    11138\n",
            "Train Data Columns: 41\n",
            "Label Column: Sim 1 Bottom Bracket X Disp.\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (12.469725909301927, -4.8756459395290515, -0.00688, 0.99169)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    9586.06 MB\n",
            "\tTrain Data (Original)  Memory Usage: 3.65 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 5 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 41 | ['Material=Steel', 'Material=Aluminum', 'Material=Titanium', 'SSB_Include', 'CSB_Include', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 36 | ['CS Length', 'BB Drop', 'Stack', 'SS E', 'ST Angle', ...]\n",
            "\t\t('int', ['bool']) :  5 | ['Material=Steel', 'Material=Aluminum', 'Material=Titanium', 'SSB_Include', 'CSB_Include']\n",
            "\t0.1s = Fit runtime\n",
            "\t41 features in original data used to generate 41 features in processed data.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting TabularPredictor for label: Sim 1 Bottom Bracket X Disp. ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\tTrain Data (Processed) Memory Usage: 3.26 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.18s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 10024, Val Rows: 1114\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ...\n",
            "\t-0.3186\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ...\n",
            "\t-0.3018\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ...\n",
            "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_set's rmse: 0.17849\n",
            "[2000]\tvalid_set's rmse: 0.176705\n",
            "[3000]\tvalid_set's rmse: 0.176325\n",
            "[4000]\tvalid_set's rmse: 0.176285\n",
            "[5000]\tvalid_set's rmse: 0.176222\n",
            "[6000]\tvalid_set's rmse: 0.17615\n",
            "[7000]\tvalid_set's rmse: 0.176122\n",
            "[8000]\tvalid_set's rmse: 0.176135\n",
            "[9000]\tvalid_set's rmse: 0.176134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-0.1761\t = Validation score   (-root_mean_squared_error)\n",
            "\t36.12s\t = Training   runtime\n",
            "\t0.89s\t = Validation runtime\n",
            "Fitting model: LightGBM ...\n",
            "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t-0.1156\t = Validation score   (-root_mean_squared_error)\n",
            "\t2.6s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ...\n",
            "\t-0.065\t = Validation score   (-root_mean_squared_error)\n",
            "\t69.74s\t = Training   runtime\n",
            "\t0.15s\t = Validation runtime\n",
            "Fitting model: CatBoost ...\n",
            "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
            "\t-0.1609\t = Validation score   (-root_mean_squared_error)\n",
            "\t4.2s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE ...\n",
            "\t-0.0621\t = Validation score   (-root_mean_squared_error)\n",
            "\t16.78s\t = Training   runtime\n",
            "\t0.15s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ...\n",
            "\t-0.0539\t = Validation score   (-root_mean_squared_error)\n",
            "\t11.87s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: XGBoost ...\n",
            "\t-0.1141\t = Validation score   (-root_mean_squared_error)\n",
            "\t3.04s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ...\n",
            "\t-0.1503\t = Validation score   (-root_mean_squared_error)\n",
            "\t23.0s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ...\n",
            "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t-0.1169\t = Validation score   (-root_mean_squared_error)\n",
            "\t12.23s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-0.0415\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.32s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 184.19s ... Best model: \"WeightedEnsemble_L2\"\n",
            "Deleting model KNeighborsUnif. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Bottom Bracket X Disp./models/KNeighborsUnif/ will be removed.\n",
            "Deleting model KNeighborsDist. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Bottom Bracket X Disp./models/KNeighborsDist/ will be removed.\n",
            "Deleting model LightGBMXT. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Bottom Bracket X Disp./models/LightGBMXT/ will be removed.\n",
            "Deleting model LightGBM. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Bottom Bracket X Disp./models/LightGBM/ will be removed.\n",
            "Deleting model RandomForestMSE. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Bottom Bracket X Disp./models/RandomForestMSE/ will be removed.\n",
            "Deleting model CatBoost. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Bottom Bracket X Disp./models/CatBoost/ will be removed.\n",
            "Deleting model XGBoost. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Bottom Bracket X Disp./models/XGBoost/ will be removed.\n",
            "Deleting model NeuralNetTorch. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Bottom Bracket X Disp./models/NeuralNetTorch/ will be removed.\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Bottom Bracket X Disp./\")\n",
            "Presets specified: ['optimize_for_deployment']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Bottom Bracket Y Disp./\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Wed Aug 30 11:19:59 UTC 2023\n",
            "Disk Space Avail:   48.33 GB / 83.96 GB (57.6%)\n",
            "Train Data Rows:    11138\n",
            "Train Data Columns: 42\n",
            "Label Column: Sim 1 Bottom Bracket Y Disp.\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (5.6847343428469665, -17.037374662996648, 0.00414, 1.00356)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    9104.9 MB\n",
            "\tTrain Data (Original)  Memory Usage: 3.74 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 5 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 42 | ['Material=Steel', 'Material=Aluminum', 'Material=Titanium', 'SSB_Include', 'CSB_Include', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 37 | ['CS Length', 'BB Drop', 'Stack', 'SS E', 'ST Angle', ...]\n",
            "\t\t('int', ['bool']) :  5 | ['Material=Steel', 'Material=Aluminum', 'Material=Titanium', 'SSB_Include', 'CSB_Include']\n",
            "\t0.1s = Fit runtime\n",
            "\t42 features in original data used to generate 42 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 3.35 MB (0.0% of available memory)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting TabularPredictor for label: Sim 1 Bottom Bracket Y Disp. ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Data preprocessing and feature engineering runtime = 0.17s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 10024, Val Rows: 1114\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ...\n",
            "\t-0.3908\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ...\n",
            "\t-0.3831\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ...\n",
            "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_set's rmse: 0.35138\n",
            "[2000]\tvalid_set's rmse: 0.347272\n",
            "[3000]\tvalid_set's rmse: 0.346965\n",
            "[4000]\tvalid_set's rmse: 0.346849\n",
            "[5000]\tvalid_set's rmse: 0.346742\n",
            "[6000]\tvalid_set's rmse: 0.34666\n",
            "[7000]\tvalid_set's rmse: 0.346643\n",
            "[8000]\tvalid_set's rmse: 0.346635\n",
            "[9000]\tvalid_set's rmse: 0.346645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-0.3466\t = Validation score   (-root_mean_squared_error)\n",
            "\t36.36s\t = Training   runtime\n",
            "\t1.04s\t = Validation runtime\n",
            "Fitting model: LightGBM ...\n",
            "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t-0.2898\t = Validation score   (-root_mean_squared_error)\n",
            "\t4.01s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ...\n",
            "\t-0.2351\t = Validation score   (-root_mean_squared_error)\n",
            "\t74.91s\t = Training   runtime\n",
            "\t0.15s\t = Validation runtime\n",
            "Fitting model: CatBoost ...\n",
            "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
            "\t-0.3046\t = Validation score   (-root_mean_squared_error)\n",
            "\t6.69s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE ...\n",
            "\t-0.2471\t = Validation score   (-root_mean_squared_error)\n",
            "\t17.23s\t = Training   runtime\n",
            "\t0.16s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ...\n",
            "\t-0.1068\t = Validation score   (-root_mean_squared_error)\n",
            "\t11.64s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: XGBoost ...\n",
            "\t-0.3202\t = Validation score   (-root_mean_squared_error)\n",
            "\t1.46s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ...\n",
            "\t-0.2939\t = Validation score   (-root_mean_squared_error)\n",
            "\t20.06s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ...\n",
            "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_set's rmse: 0.269673\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-0.2695\t = Validation score   (-root_mean_squared_error)\n",
            "\t20.25s\t = Training   runtime\n",
            "\t0.25s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-0.0878\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.51s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 199.22s ... Best model: \"WeightedEnsemble_L2\"\n",
            "Deleting model KNeighborsUnif. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Bottom Bracket Y Disp./models/KNeighborsUnif/ will be removed.\n",
            "Deleting model KNeighborsDist. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Bottom Bracket Y Disp./models/KNeighborsDist/ will be removed.\n",
            "Deleting model LightGBMXT. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Bottom Bracket Y Disp./models/LightGBMXT/ will be removed.\n",
            "Deleting model LightGBM. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Bottom Bracket Y Disp./models/LightGBM/ will be removed.\n",
            "Deleting model CatBoost. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Bottom Bracket Y Disp./models/CatBoost/ will be removed.\n",
            "Deleting model ExtraTreesMSE. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Bottom Bracket Y Disp./models/ExtraTreesMSE/ will be removed.\n",
            "Deleting model NeuralNetTorch. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Bottom Bracket Y Disp./models/NeuralNetTorch/ will be removed.\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Bottom Bracket Y Disp./\")\n",
            "Presets specified: ['optimize_for_deployment']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20231016_092811/Predictor_Sim 2 Bottom Bracket Z Disp./\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Wed Aug 30 11:19:59 UTC 2023\n",
            "Disk Space Avail:   48.09 GB / 83.96 GB (57.3%)\n",
            "Train Data Rows:    11138\n",
            "Train Data Columns: 43\n",
            "Label Column: Sim 2 Bottom Bracket Z Disp.\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (59.6124648888891, -1.5154313626269182, 0.00088, 1.04073)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    9059.91 MB\n",
            "\tTrain Data (Original)  Memory Usage: 3.83 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 5 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting TabularPredictor for label: Sim 2 Bottom Bracket Z Disp. ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 43 | ['Material=Steel', 'Material=Aluminum', 'Material=Titanium', 'SSB_Include', 'CSB_Include', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 38 | ['CS Length', 'BB Drop', 'Stack', 'SS E', 'ST Angle', ...]\n",
            "\t\t('int', ['bool']) :  5 | ['Material=Steel', 'Material=Aluminum', 'Material=Titanium', 'SSB_Include', 'CSB_Include']\n",
            "\t0.2s = Fit runtime\n",
            "\t43 features in original data used to generate 43 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 3.44 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.24s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 10024, Val Rows: 1114\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ...\n",
            "\t-0.5173\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.03s\t = Training   runtime\n",
            "\t0.14s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ...\n",
            "\t-0.4976\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.03s\t = Training   runtime\n",
            "\t0.27s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ...\n",
            "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_set's rmse: 0.273899\n",
            "[2000]\tvalid_set's rmse: 0.270414\n",
            "[3000]\tvalid_set's rmse: 0.268822\n",
            "[4000]\tvalid_set's rmse: 0.268107\n",
            "[5000]\tvalid_set's rmse: 0.26762\n",
            "[6000]\tvalid_set's rmse: 0.267409\n",
            "[7000]\tvalid_set's rmse: 0.267397\n",
            "[8000]\tvalid_set's rmse: 0.26734\n",
            "[9000]\tvalid_set's rmse: 0.267309\n",
            "[10000]\tvalid_set's rmse: 0.267288\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-0.2673\t = Validation score   (-root_mean_squared_error)\n",
            "\t40.04s\t = Training   runtime\n",
            "\t1.26s\t = Validation runtime\n",
            "Fitting model: LightGBM ...\n",
            "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t-0.281\t = Validation score   (-root_mean_squared_error)\n",
            "\t3.66s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ...\n",
            "\t-0.3268\t = Validation score   (-root_mean_squared_error)\n",
            "\t87.64s\t = Training   runtime\n",
            "\t0.15s\t = Validation runtime\n",
            "Fitting model: CatBoost ...\n",
            "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
            "\t-0.2808\t = Validation score   (-root_mean_squared_error)\n",
            "\t10.2s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE ...\n",
            "\t-0.2968\t = Validation score   (-root_mean_squared_error)\n",
            "\t15.81s\t = Training   runtime\n",
            "\t0.28s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ...\n",
            "\t-0.2559\t = Validation score   (-root_mean_squared_error)\n",
            "\t10.32s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: XGBoost ...\n",
            "\t-0.2427\t = Validation score   (-root_mean_squared_error)\n",
            "\t5.5s\t = Training   runtime\n",
            "\t0.18s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ...\n",
            "\t-0.1966\t = Validation score   (-root_mean_squared_error)\n",
            "\t29.32s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ...\n",
            "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t-0.3161\t = Validation score   (-root_mean_squared_error)\n",
            "\t12.56s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-0.1597\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.34s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 221.02s ... Best model: \"WeightedEnsemble_L2\"\n",
            "Deleting model KNeighborsUnif. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 2 Bottom Bracket Z Disp./models/KNeighborsUnif/ will be removed.\n",
            "Deleting model KNeighborsDist. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 2 Bottom Bracket Z Disp./models/KNeighborsDist/ will be removed.\n",
            "Deleting model RandomForestMSE. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 2 Bottom Bracket Z Disp./models/RandomForestMSE/ will be removed.\n",
            "Deleting model CatBoost. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 2 Bottom Bracket Z Disp./models/CatBoost/ will be removed.\n",
            "Deleting model ExtraTreesMSE. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 2 Bottom Bracket Z Disp./models/ExtraTreesMSE/ will be removed.\n",
            "Deleting model LightGBMLarge. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 2 Bottom Bracket Z Disp./models/LightGBMLarge/ will be removed.\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231016_092811/Predictor_Sim 2 Bottom Bracket Z Disp./\")\n",
            "Presets specified: ['optimize_for_deployment']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20231016_092811/Predictor_Sim 3 Bottom Bracket Y Disp./\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Wed Aug 30 11:19:59 UTC 2023\n",
            "Disk Space Avail:   48.05 GB / 83.96 GB (57.2%)\n",
            "Train Data Rows:    11138\n",
            "Train Data Columns: 44\n",
            "Label Column: Sim 3 Bottom Bracket Y Disp.\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (2.6929144951244286, -44.430876897048506, -0.00057, 1.0291)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    9374.7 MB\n",
            "\tTrain Data (Original)  Memory Usage: 3.92 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 5 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 44 | ['Material=Steel', 'Material=Aluminum', 'Material=Titanium', 'SSB_Include', 'CSB_Include', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 39 | ['CS Length', 'BB Drop', 'Stack', 'SS E', 'ST Angle', ...]\n",
            "\t\t('int', ['bool']) :  5 | ['Material=Steel', 'Material=Aluminum', 'Material=Titanium', 'SSB_Include', 'CSB_Include']\n",
            "\t0.1s = Fit runtime\n",
            "\t44 features in original data used to generate 44 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 3.53 MB (0.0% of available memory)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting TabularPredictor for label: Sim 3 Bottom Bracket Y Disp. ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Data preprocessing and feature engineering runtime = 0.17s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 10024, Val Rows: 1114\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ...\n",
            "\t-0.502\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.03s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ...\n",
            "\t-0.4896\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ...\n",
            "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t-0.3875\t = Validation score   (-root_mean_squared_error)\n",
            "\t2.95s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: LightGBM ...\n",
            "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t-0.3385\t = Validation score   (-root_mean_squared_error)\n",
            "\t4.29s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ...\n",
            "\t-0.4463\t = Validation score   (-root_mean_squared_error)\n",
            "\t94.86s\t = Training   runtime\n",
            "\t0.27s\t = Validation runtime\n",
            "Fitting model: CatBoost ...\n",
            "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
            "\t-0.4226\t = Validation score   (-root_mean_squared_error)\n",
            "\t3.18s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE ...\n",
            "\t-0.3228\t = Validation score   (-root_mean_squared_error)\n",
            "\t17.71s\t = Training   runtime\n",
            "\t0.15s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ...\n",
            "\t-0.2807\t = Validation score   (-root_mean_squared_error)\n",
            "\t9.92s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: XGBoost ...\n",
            "\t-0.4897\t = Validation score   (-root_mean_squared_error)\n",
            "\t3.41s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ...\n",
            "\t-0.3741\t = Validation score   (-root_mean_squared_error)\n",
            "\t22.4s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ...\n",
            "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t-0.4536\t = Validation score   (-root_mean_squared_error)\n",
            "\t7.3s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-0.2514\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.51s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 168.91s ... Best model: \"WeightedEnsemble_L2\"\n",
            "Deleting model KNeighborsUnif. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 3 Bottom Bracket Y Disp./models/KNeighborsUnif/ will be removed.\n",
            "Deleting model KNeighborsDist. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 3 Bottom Bracket Y Disp./models/KNeighborsDist/ will be removed.\n",
            "Deleting model LightGBMXT. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 3 Bottom Bracket Y Disp./models/LightGBMXT/ will be removed.\n",
            "Deleting model LightGBM. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 3 Bottom Bracket Y Disp./models/LightGBM/ will be removed.\n",
            "Deleting model RandomForestMSE. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 3 Bottom Bracket Y Disp./models/RandomForestMSE/ will be removed.\n",
            "Deleting model CatBoost. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 3 Bottom Bracket Y Disp./models/CatBoost/ will be removed.\n",
            "Deleting model XGBoost. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 3 Bottom Bracket Y Disp./models/XGBoost/ will be removed.\n",
            "Deleting model NeuralNetTorch. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 3 Bottom Bracket Y Disp./models/NeuralNetTorch/ will be removed.\n",
            "Deleting model LightGBMLarge. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 3 Bottom Bracket Y Disp./models/LightGBMLarge/ will be removed.\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231016_092811/Predictor_Sim 3 Bottom Bracket Y Disp./\")\n",
            "Presets specified: ['optimize_for_deployment']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20231016_092811/Predictor_Sim 3 Bottom Bracket X Rot./\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Wed Aug 30 11:19:59 UTC 2023\n",
            "Disk Space Avail:   47.82 GB / 83.96 GB (57.0%)\n",
            "Train Data Rows:    11138\n",
            "Train Data Columns: 45\n",
            "Label Column: Sim 3 Bottom Bracket X Rot.\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (48.91830899322327, -12.705045869386844, 0.0003, 1.04527)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    9337.13 MB\n",
            "\tTrain Data (Original)  Memory Usage: 4.01 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 5 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting TabularPredictor for label: Sim 3 Bottom Bracket X Rot. ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 45 | ['Material=Steel', 'Material=Aluminum', 'Material=Titanium', 'SSB_Include', 'CSB_Include', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 40 | ['CS Length', 'BB Drop', 'Stack', 'SS E', 'ST Angle', ...]\n",
            "\t\t('int', ['bool']) :  5 | ['Material=Steel', 'Material=Aluminum', 'Material=Titanium', 'SSB_Include', 'CSB_Include']\n",
            "\t0.2s = Fit runtime\n",
            "\t45 features in original data used to generate 45 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 3.62 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.24s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 10024, Val Rows: 1114\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ...\n",
            "\t-0.6328\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.04s\t = Training   runtime\n",
            "\t0.3s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ...\n",
            "\t-0.6042\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.04s\t = Training   runtime\n",
            "\t0.33s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ...\n",
            "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t-0.4177\t = Validation score   (-root_mean_squared_error)\n",
            "\t3.35s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: LightGBM ...\n",
            "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t-0.4196\t = Validation score   (-root_mean_squared_error)\n",
            "\t2.69s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ...\n",
            "\t-0.4244\t = Validation score   (-root_mean_squared_error)\n",
            "\t96.56s\t = Training   runtime\n",
            "\t0.15s\t = Validation runtime\n",
            "Fitting model: CatBoost ...\n",
            "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
            "\t-0.4228\t = Validation score   (-root_mean_squared_error)\n",
            "\t13.46s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE ...\n",
            "\t-0.4267\t = Validation score   (-root_mean_squared_error)\n",
            "\t17.96s\t = Training   runtime\n",
            "\t0.15s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ...\n",
            "\t-0.3897\t = Validation score   (-root_mean_squared_error)\n",
            "\t10.96s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: XGBoost ...\n",
            "\t-0.3962\t = Validation score   (-root_mean_squared_error)\n",
            "\t5.57s\t = Training   runtime\n",
            "\t0.12s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ...\n",
            "\t-0.4183\t = Validation score   (-root_mean_squared_error)\n",
            "\t16.57s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ...\n",
            "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_set's rmse: 0.381511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-0.3814\t = Validation score   (-root_mean_squared_error)\n",
            "\t37.44s\t = Training   runtime\n",
            "\t0.42s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-0.334\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.35s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 208.6s ... Best model: \"WeightedEnsemble_L2\"\n",
            "Deleting model KNeighborsUnif. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 3 Bottom Bracket X Rot./models/KNeighborsUnif/ will be removed.\n",
            "Deleting model KNeighborsDist. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 3 Bottom Bracket X Rot./models/KNeighborsDist/ will be removed.\n",
            "Deleting model LightGBM. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 3 Bottom Bracket X Rot./models/LightGBM/ will be removed.\n",
            "Deleting model RandomForestMSE. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 3 Bottom Bracket X Rot./models/RandomForestMSE/ will be removed.\n",
            "Deleting model CatBoost. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 3 Bottom Bracket X Rot./models/CatBoost/ will be removed.\n",
            "Deleting model ExtraTreesMSE. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 3 Bottom Bracket X Rot./models/ExtraTreesMSE/ will be removed.\n",
            "Deleting model NeuralNetTorch. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 3 Bottom Bracket X Rot./models/NeuralNetTorch/ will be removed.\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231016_092811/Predictor_Sim 3 Bottom Bracket X Rot./\")\n",
            "Presets specified: ['optimize_for_deployment']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Safety Factor (Inverted)/\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Wed Aug 30 11:19:59 UTC 2023\n",
            "Disk Space Avail:   47.79 GB / 83.96 GB (56.9%)\n",
            "Train Data Rows:    11138\n",
            "Train Data Columns: 46\n",
            "Label Column: Sim 1 Safety Factor (Inverted)\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (89.63932055031486, -0.3939172743614557, 0.00474, 1.10523)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    9408.52 MB\n",
            "\tTrain Data (Original)  Memory Usage: 4.1 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 5 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 46 | ['Material=Steel', 'Material=Aluminum', 'Material=Titanium', 'SSB_Include', 'CSB_Include', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 41 | ['CS Length', 'BB Drop', 'Stack', 'SS E', 'ST Angle', ...]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting TabularPredictor for label: Sim 1 Safety Factor (Inverted) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t\t('int', ['bool']) :  5 | ['Material=Steel', 'Material=Aluminum', 'Material=Titanium', 'SSB_Include', 'CSB_Include']\n",
            "\t0.1s = Fit runtime\n",
            "\t46 features in original data used to generate 46 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 3.71 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.18s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 10024, Val Rows: 1114\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ...\n",
            "\t-2.6039\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.03s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ...\n",
            "\t-2.6021\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.04s\t = Training   runtime\n",
            "\t0.34s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ...\n",
            "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_set's rmse: 2.55636\n",
            "[2000]\tvalid_set's rmse: 2.54448\n",
            "[3000]\tvalid_set's rmse: 2.54223\n",
            "[4000]\tvalid_set's rmse: 2.54138\n",
            "[5000]\tvalid_set's rmse: 2.54082\n",
            "[6000]\tvalid_set's rmse: 2.54058\n",
            "[7000]\tvalid_set's rmse: 2.54046\n",
            "[8000]\tvalid_set's rmse: 2.54039\n",
            "[9000]\tvalid_set's rmse: 2.54029\n",
            "[10000]\tvalid_set's rmse: 2.54031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-2.5403\t = Validation score   (-root_mean_squared_error)\n",
            "\t43.65s\t = Training   runtime\n",
            "\t1.21s\t = Validation runtime\n",
            "Fitting model: LightGBM ...\n",
            "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_set's rmse: 2.53594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-2.5352\t = Validation score   (-root_mean_squared_error)\n",
            "\t8.36s\t = Training   runtime\n",
            "\t0.23s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ...\n",
            "\t-2.575\t = Validation score   (-root_mean_squared_error)\n",
            "\t133.67s\t = Training   runtime\n",
            "\t0.17s\t = Validation runtime\n",
            "Fitting model: CatBoost ...\n",
            "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
            "\t-2.5542\t = Validation score   (-root_mean_squared_error)\n",
            "\t4.21s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE ...\n",
            "\t-2.5704\t = Validation score   (-root_mean_squared_error)\n",
            "\t19.74s\t = Training   runtime\n",
            "\t0.16s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ...\n",
            "\t-1.8275\t = Validation score   (-root_mean_squared_error)\n",
            "\t11.7s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: XGBoost ...\n",
            "\t-2.5669\t = Validation score   (-root_mean_squared_error)\n",
            "\t5.93s\t = Training   runtime\n",
            "\t0.14s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ...\n",
            "\t-2.5088\t = Validation score   (-root_mean_squared_error)\n",
            "\t19.66s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ...\n",
            "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t-2.5284\t = Validation score   (-root_mean_squared_error)\n",
            "\t8.77s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-1.8275\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.33s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 261.7s ... Best model: \"WeightedEnsemble_L2\"\n",
            "Deleting model KNeighborsUnif. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Safety Factor (Inverted)/models/KNeighborsUnif/ will be removed.\n",
            "Deleting model KNeighborsDist. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Safety Factor (Inverted)/models/KNeighborsDist/ will be removed.\n",
            "Deleting model LightGBMXT. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Safety Factor (Inverted)/models/LightGBMXT/ will be removed.\n",
            "Deleting model LightGBM. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Safety Factor (Inverted)/models/LightGBM/ will be removed.\n",
            "Deleting model RandomForestMSE. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Safety Factor (Inverted)/models/RandomForestMSE/ will be removed.\n",
            "Deleting model CatBoost. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Safety Factor (Inverted)/models/CatBoost/ will be removed.\n",
            "Deleting model ExtraTreesMSE. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Safety Factor (Inverted)/models/ExtraTreesMSE/ will be removed.\n",
            "Deleting model XGBoost. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Safety Factor (Inverted)/models/XGBoost/ will be removed.\n",
            "Deleting model NeuralNetTorch. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Safety Factor (Inverted)/models/NeuralNetTorch/ will be removed.\n",
            "Deleting model LightGBMLarge. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Safety Factor (Inverted)/models/LightGBMLarge/ will be removed.\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231016_092811/Predictor_Sim 1 Safety Factor (Inverted)/\")\n",
            "Presets specified: ['optimize_for_deployment']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20231016_092811/Predictor_Sim 3 Safety Factor (Inverted)/\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Wed Aug 30 11:19:59 UTC 2023\n",
            "Disk Space Avail:   47.79 GB / 83.96 GB (56.9%)\n",
            "Train Data Rows:    11138\n",
            "Train Data Columns: 47\n",
            "Label Column: Sim 3 Safety Factor (Inverted)\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (87.8663152959725, -0.37389303855875977, 0.00536, 1.11318)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    9343.99 MB\n",
            "\tTrain Data (Original)  Memory Usage: 4.19 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 5 features to boolean dtype as they only contain 2 unique values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting TabularPredictor for label: Sim 3 Safety Factor (Inverted) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 47 | ['Material=Steel', 'Material=Aluminum', 'Material=Titanium', 'SSB_Include', 'CSB_Include', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 42 | ['CS Length', 'BB Drop', 'Stack', 'SS E', 'ST Angle', ...]\n",
            "\t\t('int', ['bool']) :  5 | ['Material=Steel', 'Material=Aluminum', 'Material=Titanium', 'SSB_Include', 'CSB_Include']\n",
            "\t0.2s = Fit runtime\n",
            "\t47 features in original data used to generate 47 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 3.8 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.26s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 10024, Val Rows: 1114\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ...\n",
            "\t-2.4159\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ...\n",
            "\t-2.4108\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ...\n",
            "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_set's rmse: 2.5472\n",
            "[2000]\tvalid_set's rmse: 2.53887\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-2.538\t = Validation score   (-root_mean_squared_error)\n",
            "\t11.59s\t = Training   runtime\n",
            "\t0.26s\t = Validation runtime\n",
            "Fitting model: LightGBM ...\n",
            "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t-2.499\t = Validation score   (-root_mean_squared_error)\n",
            "\t3.84s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ...\n",
            "\t-2.4316\t = Validation score   (-root_mean_squared_error)\n",
            "\t113.01s\t = Training   runtime\n",
            "\t0.16s\t = Validation runtime\n",
            "Fitting model: CatBoost ...\n",
            "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
            "\t-2.5951\t = Validation score   (-root_mean_squared_error)\n",
            "\t6.31s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE ...\n",
            "\t-2.3696\t = Validation score   (-root_mean_squared_error)\n",
            "\t20.99s\t = Training   runtime\n",
            "\t0.27s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ...\n",
            "No improvement since epoch 3: early stopping\n",
            "\t-0.709\t = Validation score   (-root_mean_squared_error)\n",
            "\t7.28s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: XGBoost ...\n",
            "\t-2.4659\t = Validation score   (-root_mean_squared_error)\n",
            "\t2.52s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ...\n",
            "\t-2.4078\t = Validation score   (-root_mean_squared_error)\n",
            "\t24.99s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ...\n",
            "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\t-2.5164\t = Validation score   (-root_mean_squared_error)\n",
            "\t13.1s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-0.709\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.59s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 207.4s ... Best model: \"WeightedEnsemble_L2\"\n",
            "Deleting model KNeighborsUnif. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 3 Safety Factor (Inverted)/models/KNeighborsUnif/ will be removed.\n",
            "Deleting model KNeighborsDist. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 3 Safety Factor (Inverted)/models/KNeighborsDist/ will be removed.\n",
            "Deleting model LightGBMXT. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 3 Safety Factor (Inverted)/models/LightGBMXT/ will be removed.\n",
            "Deleting model LightGBM. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 3 Safety Factor (Inverted)/models/LightGBM/ will be removed.\n",
            "Deleting model RandomForestMSE. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 3 Safety Factor (Inverted)/models/RandomForestMSE/ will be removed.\n",
            "Deleting model CatBoost. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 3 Safety Factor (Inverted)/models/CatBoost/ will be removed.\n",
            "Deleting model ExtraTreesMSE. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 3 Safety Factor (Inverted)/models/ExtraTreesMSE/ will be removed.\n",
            "Deleting model XGBoost. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 3 Safety Factor (Inverted)/models/XGBoost/ will be removed.\n",
            "Deleting model NeuralNetTorch. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 3 Safety Factor (Inverted)/models/NeuralNetTorch/ will be removed.\n",
            "Deleting model LightGBMLarge. All files under AutogluonModels/ag-20231016_092811/Predictor_Sim 3 Safety Factor (Inverted)/models/LightGBMLarge/ will be removed.\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231016_092811/Predictor_Sim 3 Safety Factor (Inverted)/\")\n",
            "Presets specified: ['optimize_for_deployment']\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20231016_092811/Predictor_Model Mass/\"\n",
            "AutoGluon Version:  0.8.2\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Wed Aug 30 11:19:59 UTC 2023\n",
            "Disk Space Avail:   47.79 GB / 83.96 GB (56.9%)\n",
            "Train Data Rows:    11138\n",
            "Train Data Columns: 48\n",
            "Label Column: Model Mass\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (6.589862539032262, -1.6897076835673173, 0.0003, 1.0019)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    9397.83 MB\n",
            "\tTrain Data (Original)  Memory Usage: 4.28 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 5 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting TabularPredictor for label: Model Mass ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 48 | ['Material=Steel', 'Material=Aluminum', 'Material=Titanium', 'SSB_Include', 'CSB_Include', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 43 | ['CS Length', 'BB Drop', 'Stack', 'SS E', 'ST Angle', ...]\n",
            "\t\t('int', ['bool']) :  5 | ['Material=Steel', 'Material=Aluminum', 'Material=Titanium', 'SSB_Include', 'CSB_Include']\n",
            "\t0.2s = Fit runtime\n",
            "\t48 features in original data used to generate 48 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 3.89 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.27s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 10024, Val Rows: 1114\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ...\n",
            "\t-0.4369\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.03s\t = Training   runtime\n",
            "\t0.15s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ...\n",
            "\t-0.3819\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.03s\t = Training   runtime\n",
            "\t0.19s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ...\n",
            "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_set's rmse: 0.130339\n",
            "[2000]\tvalid_set's rmse: 0.121286\n",
            "[3000]\tvalid_set's rmse: 0.118726\n",
            "[4000]\tvalid_set's rmse: 0.117482\n",
            "[5000]\tvalid_set's rmse: 0.117009\n",
            "[6000]\tvalid_set's rmse: 0.116717\n",
            "[7000]\tvalid_set's rmse: 0.116537\n",
            "[8000]\tvalid_set's rmse: 0.11647\n",
            "[9000]\tvalid_set's rmse: 0.116422\n",
            "[10000]\tvalid_set's rmse: 0.1164\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-0.1164\t = Validation score   (-root_mean_squared_error)\n",
            "\t42.36s\t = Training   runtime\n",
            "\t1.17s\t = Validation runtime\n",
            "Fitting model: LightGBM ...\n",
            "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_set's rmse: 0.137374\n",
            "[2000]\tvalid_set's rmse: 0.13594\n",
            "[3000]\tvalid_set's rmse: 0.135669\n",
            "[4000]\tvalid_set's rmse: 0.135432\n",
            "[5000]\tvalid_set's rmse: 0.135351\n",
            "[6000]\tvalid_set's rmse: 0.135316\n",
            "[7000]\tvalid_set's rmse: 0.135304\n",
            "[8000]\tvalid_set's rmse: 0.135294\n",
            "[9000]\tvalid_set's rmse: 0.135288\n",
            "[10000]\tvalid_set's rmse: 0.135284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-0.1353\t = Validation score   (-root_mean_squared_error)\n",
            "\t63.13s\t = Training   runtime\n",
            "\t1.21s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ...\n",
            "\t-0.2241\t = Validation score   (-root_mean_squared_error)\n",
            "\t80.58s\t = Training   runtime\n",
            "\t0.24s\t = Validation runtime\n",
            "Fitting model: CatBoost ...\n",
            "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
            "\t-0.1399\t = Validation score   (-root_mean_squared_error)\n",
            "\t70.78s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE ...\n",
            "\t-0.2043\t = Validation score   (-root_mean_squared_error)\n",
            "\t16.71s\t = Training   runtime\n",
            "\t0.24s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ...\n",
            "\t-0.1258\t = Validation score   (-root_mean_squared_error)\n",
            "\t10.89s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: XGBoost ...\n",
            "\t-0.1416\t = Validation score   (-root_mean_squared_error)\n",
            "\t7.2s\t = Training   runtime\n",
            "\t0.32s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ...\n",
            "\t-0.142\t = Validation score   (-root_mean_squared_error)\n",
            "\t28.55s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ...\n",
            "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
            "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\tvalid_set's rmse: 0.144699\n",
            "[2000]\tvalid_set's rmse: 0.144461\n",
            "[3000]\tvalid_set's rmse: 0.144442\n",
            "[4000]\tvalid_set's rmse: 0.144439\n",
            "[5000]\tvalid_set's rmse: 0.144439\n",
            "[6000]\tvalid_set's rmse: 0.144438\n",
            "[7000]\tvalid_set's rmse: 0.144438\n",
            "[8000]\tvalid_set's rmse: 0.144438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-0.1444\t = Validation score   (-root_mean_squared_error)\n",
            "\t164.99s\t = Training   runtime\n",
            "\t1.68s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-0.1045\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.32s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 499.85s ... Best model: \"WeightedEnsemble_L2\"\n",
            "Deleting model KNeighborsUnif. All files under AutogluonModels/ag-20231016_092811/Predictor_Model Mass/models/KNeighborsUnif/ will be removed.\n",
            "Deleting model KNeighborsDist. All files under AutogluonModels/ag-20231016_092811/Predictor_Model Mass/models/KNeighborsDist/ will be removed.\n",
            "Deleting model RandomForestMSE. All files under AutogluonModels/ag-20231016_092811/Predictor_Model Mass/models/RandomForestMSE/ will be removed.\n",
            "Deleting model CatBoost. All files under AutogluonModels/ag-20231016_092811/Predictor_Model Mass/models/CatBoost/ will be removed.\n",
            "Deleting model ExtraTreesMSE. All files under AutogluonModels/ag-20231016_092811/Predictor_Model Mass/models/ExtraTreesMSE/ will be removed.\n",
            "Deleting model XGBoost. All files under AutogluonModels/ag-20231016_092811/Predictor_Model Mass/models/XGBoost/ will be removed.\n",
            "Deleting model LightGBMLarge. All files under AutogluonModels/ag-20231016_092811/Predictor_Model Mass/models/LightGBMLarge/ will be removed.\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231016_092811/Predictor_Model Mass/\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultilabelPredictor saved to disk. Load with: MultilabelPredictor.load('AutogluonModels/ag-20231016_092811/')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_predictor = MultilabelPredictor.load(\"AutogluonModels/ag-20231016_092811/\")\n",
        "predictions = my_predictor.predict(x_test)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "\n",
        "r2, mse, mae"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdQjKO1ZhGuz",
        "outputId": "24802ae6-7db0-43dc-8baa-28466779f00f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7004860364064162, 0.180076212466393, 0.12404996870644322)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.make_archive(\"trained-model\", \"zip\", \"/content/AutogluonModels\")\n"
      ],
      "metadata": {
        "id": "yHX6e5pJ-KdH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5297e788-13a1-4a23-d201-34d074727df1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/trained-model.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4-uVflVSDr1E"
      }
    }
  ]
}